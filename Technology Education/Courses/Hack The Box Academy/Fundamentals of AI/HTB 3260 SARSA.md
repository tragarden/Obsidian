# SARSA

State-Action-Reward-State-Action (SARSA) is a model-free reinforcement learning algorithm based on environmental interactions. SARSA updates Q-values based on the next and the action that would be taken in that state.

SARSA is an on-policy algorithm that learns the values of the policy it is currently using. 

Alternatively, Q-learning is off-policy and learns the value of the optimal policy independent of the current policy it is using.

The update rule is below:

>Q(s, a) <- Q (s, a) + a \* (r + y \* Q(s', a') - Q(s, a))

- s is the current state
- a is the current action
- r is the reward
- s' is the next state
- a' is the next action
- alpha is the learning rate
- y is the discount factor
- Q(s', a') is the expected future reward for the next state-action pair

There are seven steps involved in the SARSA algorithm:

1. Initialization - the Q-table is created with arbitrary values or zeroes for each state-action pair. This table stores the estimated Q-values for actions in different states.
2. Choose Actions - during the current state s, an action a is selected to be executed. This is usually based on exploration-exploitation strategies like epsilon-greedy to find balance between exploration and exploitation.
3. Perform Actions - execute action a and observe the next state s' and the reward r received. 
4. Choose a' - during the next state s', select the next action a' based on the current policy. Considering the next action is a key component of the on-policy nature of SARSA.
5. Update Q-values - update the values for the state-action pairs.
6. Update s and a - update the current state and action to the next state s' and action a'. This readies the algorithm for the next iteration of content.
7. Iterate - repeat the previous steps until Q-values converge or the max number of iterations is reached. This is a key feature for policy refinement.

#### On and Off Policies

On-Policy Learning is when the algorithm learns the values of it's current policy. The rewards and actions of this policy influence updates.

Off-Policy Learning is when the algorithm learns about optimal plicy independently of the policy being followed. This allows learning from data generated by other policies to the benefit of the current policy.

On-policy learning is beneficial when we value stability and safety. When SARSA is following it's current policy, this means it is less likely to explore dangerous or unstable actions.

As an exploration method, SARSA is a good balance between exploitation and exploration.

You can think of SARSA as an on-the-job learning algorithm that uses experience to develop policy.

#### Softmax

Softmax is a method that assigns probabilities to actions based on their Q-values. 