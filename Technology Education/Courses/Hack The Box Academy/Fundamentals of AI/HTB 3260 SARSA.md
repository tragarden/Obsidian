# SARSA

State-Action-Reward-State-Action (SARSA) is a model-free reinforcement learning algorithm based on environmental interactions. SARSA updates Q-values based on the next and the action that would be taken in that state.

SARSA is an on-policy algorithm that learns the values of the policy it is currently using. 

Alternatively, Q-learning is off-policy and learns the value of the optimal policy independent of the current policy it is using.

The update rule is below:

>Q(s, a) <- Q (s, a) + a \* (r + y \* Q(s', a') - Q(s, a))

- s is the current state
- a is the current action
- r is the reward
- s' is the next state
- a' is the next action
- alpha is the learning rate
- y is the discount factor
- Q(s', a') is the expected future reward for the next state-action pair

There are seven steps involved in the SARSA algorithm:

1. Initialization - the Q-table is created with arbitrary values or zeroes for each state-action pair. This table stores the estimated Q-values for actions in different states.
2. Choose Actions - during the current state s, an action a is selected to be executed. This is usually based on exploration-exploitation strategies like epsilon-greedy to find balance between exploration and exploitation.
3. Perform Actions - execute action a and observe the next state s' and the reward r received. 
4. Choose a' - during the next state s', select the next action a' based on the current policy. Considering the next action is a key component of the on-policy nature of SARSA.
5. Update Q-values - update the values for the state-action pairs.
6. Update s and a - update the current state and action to the next state s' and action a'. Thisreadies the algorithm for the next iteration of content.
7. Iterate - repeat the previous steps until Q-values converge or the max number of iterations is reached. This is a key feature for policy refinement.

#### On and Off Policies

On-Policy Learning is when the algorithm learns the values of it's current policy. The rewards and actions of this policy influence updates.

Off-Policy Learning is when the algorithm learns about optimal plicy independently of the policy being followed. This allows learning from data generated by other policies to the benefit of the current policy.

On-policy learning is beneficial when we value stability and safety. When SARSA is following it's current policy, this means it is less likely to explore dangerous or unstable actions.

As an exploration method, SARSA is a good balance between exploitation and exploration.

You can think of SARSA as an on-the-job learning algorithm that uses experience to develop policy.

#### Softmax

Softmax is a method that assigns probabilities to actions based on their Q-values, where higher Q-values are indicative of higher probabilities. This is a flexible method that encourages more nuanced and adaptive behaviors by encouraging the agent to make higher risk decisions

#### Convergence 

Convergence in the context of Reinforcement Learning means that a level of stability has been attained where Q-values remain relatively unchanging during new iterations and the policy has maximized rewards.

Parameters that greatly influence convergence are:

- Learning Rate (a) - controls how much Q-values are updated during each iteration. Higher a means faster updating but less stability. Lower a means more convergence but a slower process.
- Discount Factor (y) - determines a balance between long-term and short-term rewards. Higher y values mean more long-term rewards while lower y means more short-term rewards.

#### Assumptions

We will make the following assumptions when using SARSA:

- Markov Property - the environment qualifies for this property, meaning that the next state is dependent on the current state and action, not past states and actions.
- Stationary Environment - environmental factors do not change.

### Related:
- [Hack The Box Academy](https://academy.hackthebox.com/ "Hack The Box Academy Home page")
- [HTB SARSA](https://academy.hackthebox.com/module/290/section/3260 "HTB SARSA")