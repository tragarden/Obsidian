# Process

I began this process by installing a fresh iteration of Python into my C:\Tools directory and verifying that Python and Pip were installed with the following commands:

```
python --version
``` 

```
pip --version
```

During this process I generated the code via GPT but added comments line by line in my own words to describe what is happening to bolster my understanding instead of blindly copy/pasting what AI told me.

### Reading File Timestamps

First I worked with ChatGPT to output some code that would read the date from a file so it can be used for YAML injection. At first I used the following block of code generated by GPT:

```
import os    # imports the os library

import datetime    # imports the datetime library

  

def get_creation_date(path):    # defines the function name and parameter

    if os.name == 'nt':  # If using Windows OS

        return datetime.datetime.fromtimestamp(os.path.getmtime(path))

    else:

        # On Unix, getctime is actually change time (last metadata change)

        # Creation date might be unavailable

        return datetime.datetime.fromtimestamp(os.path.getmtime(path))

  

file_path = r"C:\Users\travi\Desktop\Obsidian\Personal Vault\Khan Academy\Math\Algebra 1\Unit 2\0201 KA Balancing Equations.md"

creation_date = get_creation_date(file_path)

# strftime is String Format Time, formatting datetime to readable string

print(f"Creation date of the file: {creation_date.strftime('%Y-%m-%d')}")
```

NOTE: Originally I used the 'getctime()' function, which returned the incorrect creation date - ironically the creation date metadata held the most recently modified timestamp, while 'getmtime()' returned the date of creation in the file.

In other words:
- getctime() was returning a more recent date than getmtime(), which returned the date I made the file.
- strftime() is used to indicate the formatting for returned dates in the f string variable call. This makes for neat and manageable application of YAML headers.

### Automate Folder Scanning and YAML

Now I used GPT to iterate through the .md files within a directory and read them - then inject the related YAML header.

Next I generated the following code via GPT:

```
import os

def get_markdown_files(directory):    # defines the function and parameter
    md_files = []    # sets the md_files variable to an empty array
    for root, dirs, files in os.walk(directory):    # os.walk iterates through files/directories
        for file in files:
            if file.endswith(".md"):    # seeks out markdown files (.md)
                full_path = os.path.join(root, file)    # sets the file path
                md_files.append(full_path) # stores known file paths to md_files
    return md_files

# Example usage
folder_path = r"C:\Users\travi\Desktop\Obsidian\Personal Vault\" # sets my directory to the folder path
markdown_files = get_markdown_files(folder_path)    # stores md files

for file in markdown_files:
    print(file)    # prints the file name for each file

```

This code will iterate through every markdown file within the indicated directory and print the file path of each file to output. When I did this I noticed some files that I don't want to be included in the output, like my Journal. I changed the file path to a more narrow scope.

I then was curious about how many pages of notes I've taken, so I asked GPT to include a print statement that utilizes f strings to return how many files were found as shown below:

```
# Print total count AFTER listing all files
print(f"\nTotal Markdown files found: {len(markdown_files)}")
```

This print statement revealed that I've authored 611 pages of notes since 2020. That is truly blowing my mind right now and I grossly underestimated how many pages I've written by more than half. This didn't even include my separate school directory for obsidian. I felt astonished and proud of my work.

### YAML Insertion 

Now I started working towards identifying any file without a YAML header (hint: all of them) before injecting the YAML header into each file via automation. I wanted to adhere to the following conditions:

- Testing on small, unimportant directory with dummy copies of the files.
- Leaving the original files unmodified.
	- Input: Original Files
- Writing the newly formatted markdown files to a separate location.
	- Output: New files separate from the original, unmodified files.

A skeletal framework is below:

```
Markdown-Formatter
	/input files/    # files to be copied
	/output files/    # YAML injected versions saved to new location
	get_markdown_files.py    # script to identify all .md files
```

I then wanted to generate a script that would take sample files from folder A as input, and create newly modified iterations of these files in folder B as the output. This is to prevent any of the original file contents from being modified to retain my original work.

This is the following code generated by GPT to accomplish this:

```
input_folder = r"C:\Users\travi\Desktop\MD Test Cases"    # assigns input dir
output_folder = r"C:\Users\travi\Desktop\MD Test Results"    # assigns output dir

markdown_files = get_markdown_files(input_folder)    # scans input dir

for file in markdown_files:    # evaluates all files from input
    # Read original
    with open(file, 'r', encoding='utf-8') as f:    # opens file, reads, encodes and sets the name to f
        content = f.read()    # reads all input content assigned to f

    # (Later we'll inject YAML here)

    # Write new version
    filename = os.path.basename(file)    # reads original file name and stores it
    output_path = os.path.join(output_folder, filename)    # defines output location iteratively

    with open(output_path, 'w', encoding='utf-8') as f:    # defines permissions when files are opened
        f.write(content)    # writes new file to the output folder

print(f"\nCompleted processing {len(markdown_files)} files.")    # determines how many files were processed and indicates completion
```

This script will simply copy the files found in the input folder unmodified to the output folder to test that this process is working safely and retains original file data.

The first thing I noticed is that this script takes the files from various directories, but in the output all files are stored within one directory. I then asked GPT how to create the subfolders as well in a step by step way so I could understand and see how the code was built. Up until this point I was simply generating code blocks, but as I'm working through them I am less intimidated by the code and understanding it more.
